{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNFF/GjHYzBgLje4NT1auIS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Capstone 3: Predicting Customer Lifetime Value (CLV)\n","\n","## Predicting Customer Lifetime Value (CLV) to Optimize Targeted Marketing Strategies in E-Commerce\n","\n","This capstone project aims to enhance targeted marketing strategies in the e-commerce sector by accurately predicting CLV for the largest online department store in Brazil, encompassing 100,000 anonymized orders made between 2016 and 2018.\n","\n","Target feature: `customer_total_spend`\n","\n","## PRE-PROCESSING, TRAINING, AND MODELING\n","\n","## Table of Contents\n","* [Feature Engineering](#feature_engineering)\n","* [One-Hot Encoding](#one_hot_encoding)\n","* [Train-Test Split](#train_test_split)\n","* [Scaling](#scaling)\n","* [Model Training and Evaluation](#model_training_evaluation)\n","  * [Baseline Model: Linear Regression](#baseline_linear)\n","      * [Non-Scaled Data](#nonscaled)\n","      * [Scaled Data](#scaled)\n","  * [Alternative Models](#alternative_models)\n","      * [Random Forest, Gradient Boosting, Lasso](#random_gradient_lasso)\n","  * [Ensemble Model](#ensemble)\n","  * [Stacked Model](#stacked)\n","* [Compare and Select the Best Model](#compare_select)\n","* [Summary](#summary)"],"metadata":{"id":"I4HMXt4Gorct"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor\n","from sklearn.linear_model import LinearRegression, Lasso\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import mean_squared_error, r2_score"],"metadata":{"id":"SNg3awp7rq5E","executionInfo":{"status":"ok","timestamp":1721598382575,"user_tz":240,"elapsed":230,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import shutil  # Make sure to import the shutil module"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x20_tPrJrtxS","executionInfo":{"status":"ok","timestamp":1721597899156,"user_tz":240,"elapsed":939,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"4bf367f4-70c2-4e93-e6e3-ac2f417aa0d4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["clv = pd.read_csv('/content/drive/My Drive/Colab Notebooks/df_clv_eda.csv')"],"metadata":{"id":"hZ8fCY1qJfk1","executionInfo":{"status":"ok","timestamp":1721597903461,"user_tz":240,"elapsed":672,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["clv.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_mB5xsRRFO7","executionInfo":{"status":"ok","timestamp":1721597904579,"user_tz":240,"elapsed":232,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"40aeee7c-f06e-4d2a-ec67-e0fb3703792a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 92062 entries, 0 to 92061\n","Data columns (total 17 columns):\n"," #   Column                               Non-Null Count  Dtype  \n","---  ------                               --------------  -----  \n"," 0   customer_unique_id                   92062 non-null  object \n"," 1   customer_average_order_value         92062 non-null  float64\n"," 2   customer_tenure_days                 92062 non-null  int64  \n"," 3   customer_recency_days                92062 non-null  int64  \n"," 4   customer_total_orders_frequency      92062 non-null  int64  \n"," 5   customer_total_spend_monetary        92062 non-null  float64\n"," 6   customer_unique_products             92062 non-null  int64  \n"," 7   customer_category_diversity          92062 non-null  int64  \n"," 8   customer_average_product_price       92062 non-null  float64\n"," 9   customer_total_units                 92062 non-null  int64  \n"," 10  customer_average_delivery_time       92062 non-null  float64\n"," 11  customer_average_shipping_cost       92062 non-null  float64\n"," 12  customer_on_time_delivery_rate       92062 non-null  float64\n"," 13  customer_preferred_payment_type      92062 non-null  object \n"," 14  customer_average_installments        92062 non-null  float64\n"," 15  customer_total_payment_transactions  92062 non-null  int64  \n"," 16  customer_first_purchase_date         92062 non-null  object \n","dtypes: float64(7), int64(7), object(3)\n","memory usage: 11.9+ MB\n"]}]},{"cell_type":"markdown","source":["## Feature Engineering <a class=\"anchor\" id=\"feature_engineering\"></a>\n","\n","While the dataset above was created from our entire original dataset and was useful for exploring our data in the EDA portion of our analysis, in the context of feature engineering and data pre-processing we know that data leakage occurs when information from outside the training set is used to create or transform features, leading to overly optimistic performance estimates during model evaluation. In short, the statistical properties of the entire dataset, including the test set, were used to create these features, which wouldn't be available in a real-world scenario when making predictions on new data.\n","\n","For example,\n","- **Customer Recency and Frequency Metrics (e.g., `customer_recency_days`, `customer_total_orders_frequency`):** These were calculated using data that includes periods extending into what becomes the test set, so there is potential leakage.\n","- **Average Metrics (e.g., `customer_average_order_value`, `customer_average_product_price`, `customer_average_shipping_cost`):** Similar to scaling, these averages were computed using the entire dataset, including the test set, so the model has seen future information.\n","\n","Therefore, we need to split our original dataset shown below into train and test subsets first, and then recreate the same features for both subsets."],"metadata":{"id":"JhM44FaOY3KC"}},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/df_eda.csv')"],"metadata":{"id":"BW-UiOB5cI8M","executionInfo":{"status":"ok","timestamp":1721598000191,"user_tz":240,"elapsed":2041,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"41YcWeZ_cxNB","executionInfo":{"status":"ok","timestamp":1721598000421,"user_tz":240,"elapsed":10,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"e742bbad-b64a-46cd-f2cc-8cd9b273c871"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 113367 entries, 0 to 113366\n","Data columns (total 32 columns):\n"," #   Column                         Non-Null Count   Dtype  \n","---  ------                         --------------   -----  \n"," 0   customer_id                    113367 non-null  object \n"," 1   customer_unique_id             113367 non-null  object \n"," 2   customer_zip_code_prefix       113367 non-null  int64  \n"," 3   customer_city                  113367 non-null  object \n"," 4   customer_state                 113367 non-null  object \n"," 5   order_id                       113367 non-null  object \n"," 6   order_status                   113367 non-null  object \n"," 7   order_purchase_timestamp       113367 non-null  object \n"," 8   order_approved_at              113367 non-null  object \n"," 9   order_delivered_carrier_date   113367 non-null  object \n"," 10  order_delivered_customer_date  113367 non-null  object \n"," 11  order_estimated_delivery_date  113367 non-null  object \n"," 12  order_item_id                  113367 non-null  int64  \n"," 13  product_id                     113367 non-null  object \n"," 14  seller_id                      113367 non-null  object \n"," 15  shipping_limit_date            113367 non-null  object \n"," 16  price                          113367 non-null  float64\n"," 17  freight_value                  113367 non-null  float64\n"," 18  product_name_lenght            113367 non-null  float64\n"," 19  product_description_lenght     113367 non-null  float64\n"," 20  product_photos_qty             113367 non-null  float64\n"," 21  product_weight_g               113367 non-null  float64\n"," 22  product_length_cm              113367 non-null  float64\n"," 23  product_height_cm              113367 non-null  float64\n"," 24  product_width_cm               113367 non-null  float64\n"," 25  payment_sequential             113367 non-null  int64  \n"," 26  payment_type                   113367 non-null  object \n"," 27  payment_installments           113367 non-null  int64  \n"," 28  payment_value                  113367 non-null  float64\n"," 29  product_category               113367 non-null  object \n"," 30  delivery_time_days             113367 non-null  int64  \n"," 31  on_time_delivery               113367 non-null  bool   \n","dtypes: bool(1), float64(10), int64(5), object(16)\n","memory usage: 26.9+ MB\n"]}]},{"cell_type":"code","source":["# Check for duplicates\n","df.duplicated().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BUwLostztxr8","executionInfo":{"status":"ok","timestamp":1721598003795,"user_tz":240,"elapsed":624,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"aac990f7-3470-452f-eec6-29080698a5a9"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["We split our original dataset into train and test subsets and recreate the same features on both."],"metadata":{"id":"roj0OInp4ORx"}},{"cell_type":"code","source":["def engineer_customer_features(df):\n","    # Convert date columns to datetime\n","    date_columns = [\n","        'order_purchase_timestamp',\n","        'order_delivered_customer_date',\n","        'order_estimated_delivery_date',\n","        'order_approved_at',\n","        'order_delivered_carrier_date'\n","    ]\n","    for col in date_columns:\n","        if col in df.columns:\n","            df[col] = pd.to_datetime(df[col])\n","\n","    # Calculate delivery time days\n","    df['delivery_time_days'] = (df['order_delivered_customer_date'] - df['order_purchase_timestamp']).dt.days\n","\n","    # Determine if deliveries were on time\n","    df['on_time_delivery'] = df['order_delivered_customer_date'] <= df['order_estimated_delivery_date']\n","\n","    # Grouping by 'customer_unique_id' and applying multiple aggregations\n","    aggregations = {\n","        'order_id': 'nunique',\n","        'payment_value': 'sum',\n","        'product_id': 'nunique',\n","        'product_category': 'nunique',\n","        'price': 'mean',\n","        'order_item_id': 'count',\n","        'delivery_time_days': 'mean',\n","        'freight_value': 'mean',\n","        'on_time_delivery': 'mean',\n","        'payment_installments': 'mean',\n","        'payment_sequential': 'sum',\n","        'order_purchase_timestamp': ['min', 'max']\n","    }\n","\n","    # Creating initial aggregates\n","    agg_df = df.groupby('customer_unique_id').agg(aggregations).reset_index()\n","\n","    # Renaming columns\n","    agg_df.columns = [\n","        'customer_unique_id',\n","        'customer_total_orders',\n","        'customer_total_spend',\n","        'customer_unique_products',\n","        'customer_category_diversity',\n","        'customer_average_product_price',\n","        'customer_total_units',\n","        'customer_average_delivery_time',\n","        'customer_average_shipping_cost',\n","        'customer_on_time_delivery_rate',\n","        'customer_average_installments',\n","        'customer_total_payment_transactions',\n","        'first_purchase_date',\n","        'last_purchase_date'\n","]\n","\n","    # Calculating additional features\n","    agg_df['customer_average_order_value'] = agg_df['customer_total_spend'] / agg_df['customer_total_orders']\n","    agg_df['customer_tenure_days'] = (agg_df['last_purchase_date'] - agg_df['first_purchase_date']).dt.days\n","    agg_df['customer_recency_days'] = (agg_df['last_purchase_date'].max() - agg_df['last_purchase_date']).dt.days\n","\n","    # Preferred Payment Method\n","    preferred_payment_method = df.groupby(['customer_unique_id', 'payment_type']).size().reset_index(name='counts')\n","    preferred_payment_method = preferred_payment_method.sort_values('counts', ascending=False).drop_duplicates('customer_unique_id')\n","    preferred_payment_method = preferred_payment_method[['customer_unique_id', 'payment_type']]\n","    preferred_payment_method = preferred_payment_method.rename(columns={'payment_type': 'customer_preferred_payment_type'})\n","\n","    # Merging the preferred payment method\n","    agg_df = agg_df.merge(preferred_payment_method, on='customer_unique_id', how='left')\n","\n","    return agg_df\n","\n","# Split the data\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","# Feature engineering on both training and test sets\n","train_features_df = engineer_customer_features(train_df)\n","test_features_df = engineer_customer_features(test_df)\n","\n","# Select only the relevant columns\n","final_columns = [\n","    'customer_unique_id', 'customer_total_orders', 'customer_total_spend', 'customer_average_order_value',\n","    'customer_tenure_days', 'customer_recency_days', 'customer_unique_products',\n","    'customer_category_diversity', 'customer_average_product_price',\n","    'customer_total_units', 'customer_average_delivery_time',\n","    'customer_average_shipping_cost', 'customer_on_time_delivery_rate',\n","    'customer_preferred_payment_type', 'customer_average_installments',\n","    'customer_total_payment_transactions'\n","]\n","\n","train_features_df = train_features_df[final_columns]\n","test_features_df = test_features_df[final_columns]"],"metadata":{"id":"8_Bt9j2Hz0jn","executionInfo":{"status":"ok","timestamp":1721598043140,"user_tz":240,"elapsed":1516,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["train_features_df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5oA-cNqC3ZUY","executionInfo":{"status":"ok","timestamp":1721598046323,"user_tz":240,"elapsed":215,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"679749fa-92d0-4d43-fbb3-24381b3b67bb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 76027 entries, 0 to 76026\n","Data columns (total 16 columns):\n"," #   Column                               Non-Null Count  Dtype  \n","---  ------                               --------------  -----  \n"," 0   customer_unique_id                   76027 non-null  object \n"," 1   customer_total_orders                76027 non-null  int64  \n"," 2   customer_total_spend                 76027 non-null  float64\n"," 3   customer_average_order_value         76027 non-null  float64\n"," 4   customer_tenure_days                 76027 non-null  int64  \n"," 5   customer_recency_days                76027 non-null  int64  \n"," 6   customer_unique_products             76027 non-null  int64  \n"," 7   customer_category_diversity          76027 non-null  int64  \n"," 8   customer_average_product_price       76027 non-null  float64\n"," 9   customer_total_units                 76027 non-null  int64  \n"," 10  customer_average_delivery_time       76027 non-null  float64\n"," 11  customer_average_shipping_cost       76027 non-null  float64\n"," 12  customer_on_time_delivery_rate       76027 non-null  float64\n"," 13  customer_preferred_payment_type      76027 non-null  object \n"," 14  customer_average_installments        76027 non-null  float64\n"," 15  customer_total_payment_transactions  76027 non-null  int64  \n","dtypes: float64(7), int64(7), object(2)\n","memory usage: 9.3+ MB\n"]}]},{"cell_type":"code","source":["test_features_df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L8-WqHge3Zd_","executionInfo":{"status":"ok","timestamp":1721598048488,"user_tz":240,"elapsed":199,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"491c544a-28b7-4340-9f4e-489b419a91a9"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 21362 entries, 0 to 21361\n","Data columns (total 16 columns):\n"," #   Column                               Non-Null Count  Dtype  \n","---  ------                               --------------  -----  \n"," 0   customer_unique_id                   21362 non-null  object \n"," 1   customer_total_orders                21362 non-null  int64  \n"," 2   customer_total_spend                 21362 non-null  float64\n"," 3   customer_average_order_value         21362 non-null  float64\n"," 4   customer_tenure_days                 21362 non-null  int64  \n"," 5   customer_recency_days                21362 non-null  int64  \n"," 6   customer_unique_products             21362 non-null  int64  \n"," 7   customer_category_diversity          21362 non-null  int64  \n"," 8   customer_average_product_price       21362 non-null  float64\n"," 9   customer_total_units                 21362 non-null  int64  \n"," 10  customer_average_delivery_time       21362 non-null  float64\n"," 11  customer_average_shipping_cost       21362 non-null  float64\n"," 12  customer_on_time_delivery_rate       21362 non-null  float64\n"," 13  customer_preferred_payment_type      21362 non-null  object \n"," 14  customer_average_installments        21362 non-null  float64\n"," 15  customer_total_payment_transactions  21362 non-null  int64  \n","dtypes: float64(7), int64(7), object(2)\n","memory usage: 2.6+ MB\n"]}]},{"cell_type":"markdown","source":["## One-Hot Encoding <a id=\"one_hot_encoding\"></a>\n","\n","Next, we apply one-hot encoding, which:\n","- Fits the encoder on the training subset only to avoid data leakage.\n","- Transforms both the training and test subsets with the fitted encoder.\n","- Creates data frames for the encoded features and merges them back into the original data frames after dropping the original column."],"metadata":{"id":"bpxNilyM6BKh"}},{"cell_type":"code","source":["# One-hot encoding\n","encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n","\n","# Fit the encoder only on training data and transform both training and test sets\n","encoded_train_features = encoder.fit_transform(train_features_df[['customer_preferred_payment_type']])\n","encoded_test_features = encoder.transform(test_features_df[['customer_preferred_payment_type']])\n","\n","# Create DataFrame with the encoded features\n","encoded_columns = encoder.get_feature_names_out(['customer_preferred_payment_type'])\n","encoded_train_df = pd.DataFrame(encoded_train_features, columns=encoded_columns, index=train_features_df.index)\n","encoded_test_df = pd.DataFrame(encoded_test_features, columns=encoded_columns, index=test_features_df.index)\n","\n","# Drop the original 'customer_preferred_payment_type' column and add the encoded features\n","train_features_df = train_features_df.drop(columns=['customer_preferred_payment_type']).join(encoded_train_df)\n","test_features_df = test_features_df.drop(columns=['customer_preferred_payment_type']).join(encoded_test_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FYrFv4mS5cQX","executionInfo":{"status":"ok","timestamp":1721598135448,"user_tz":240,"elapsed":216,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"5e3c2612-2705-494c-896b-994366044dd9"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["## Train-Test Split <a id=\"train_test_split\"></a>\n","\n","Now we drop non-relevant features. In this case, we only have one."],"metadata":{"id":"Q-52gm3P74cY"}},{"cell_type":"code","source":["# Drop non-relevant features\n","train_features_df = train_features_df.drop(columns=['customer_unique_id'])\n","test_features_df = test_features_df.drop(columns=['customer_unique_id'])"],"metadata":{"id":"UDr61ydskVXB","executionInfo":{"status":"ok","timestamp":1721598168170,"user_tz":240,"elapsed":199,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["We split the data (non-scaled) for model evaluation."],"metadata":{"id":"h1Ql8v3Mu6Yw"}},{"cell_type":"code","source":["# Test-train split\n","target_column_name = 'customer_total_spend'\n","\n","X_train = train_features_df.drop(columns=[target_column_name])\n","y_train = train_features_df[target_column_name]\n","X_test = test_features_df.drop(columns=[target_column_name])\n","y_test = test_features_df[target_column_name]"],"metadata":{"id":"ISsBZKSP5cHe","executionInfo":{"status":"ok","timestamp":1721598187193,"user_tz":240,"elapsed":204,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## Scaling <a class=\"anchor\" id=\"scaling\"></a>\n","\n","Then we scale the data using standardization."],"metadata":{"id":"1NNTTJaQjhCF"}},{"cell_type":"code","source":["# Initialize the scaler\n","scaler = StandardScaler()\n","\n","# Fit the scaler on the training data and transform both train and test data\n","scaled_X_train = scaler.fit_transform(X_train)\n","scaled_X_test = scaler.transform(X_test)"],"metadata":{"id":"hPKEJAlJjjgW","executionInfo":{"status":"ok","timestamp":1721598204042,"user_tz":240,"elapsed":175,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## Model Training and Evaluation <a id=\"model_training_evaluation\"></a>\n","\n","### Linear Regression <a id=\"linear regression\"></a>\n","\n","We will use linear regression as our baseline model. We will also use this model to evaluate how the model performs using scaled and non-scaled data.\n","\n","#### Non-Scaled Data <a id=\"nonscaled\"></a>\n","\n","First, we evaluate the model using non-scaled data."],"metadata":{"id":"5yT0dpp-pWmG"}},{"cell_type":"code","source":["# Initialize the linear regression model.\n","lr = LinearRegression()\n","\n","# Train and evaluate on non-scaled data\n","lr.fit(X_train, y_train)\n","y_pred_non_scaled = lr.predict(X_test)\n","lr_mse_non_scaled = mean_squared_error(y_test, y_pred_non_scaled)\n","lr_r2_non_scaled = r2_score(y_test, y_pred_non_scaled)\n","\n","print(\"MSE with non-scaled data:\", lr_mse_non_scaled)\n","print(\"R² with non-scaled data:\", lr_r2_non_scaled)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7EnPc9K8pazo","executionInfo":{"status":"ok","timestamp":1721598251101,"user_tz":240,"elapsed":593,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"1c377a88-379a-4c38-ae52-45d99e95b8e8"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE with non-scaled data: 2352.7929843740335\n","R² with non-scaled data: 0.9821246397014615\n"]}]},{"cell_type":"markdown","source":["#### Scaled Data <a id=\"scaled\"></a>\n","\n","Then we train and evaluate linear regression on scaled data."],"metadata":{"id":"wwI0h7EKpeTQ"}},{"cell_type":"code","source":["# Train and evaluate on scaled data\n","lr.fit(scaled_X_train, y_train)\n","y_pred_scaled = lr.predict(scaled_X_test)\n","lr_mse_scaled = mean_squared_error(y_test, y_pred_scaled)\n","lr_r2_scaled = r2_score(y_test, y_pred_scaled)\n","\n","print(\"MSE with scaled data:\", lr_mse_scaled)\n","print(\"R² with scaled data:\", lr_r2_scaled)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gyd5HvPqpoPE","executionInfo":{"status":"ok","timestamp":1721598275624,"user_tz":240,"elapsed":198,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"f84549e5-b70e-4b5f-ed81-72ba011975fc"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE with scaled data: 2352.7929843740417\n","R² with scaled data: 0.9821246397014615\n"]}]},{"cell_type":"markdown","source":["We see that the same scores were returned for both non-scaled and scaled data.\n","\n","Below, we do a final check and confirm that the data was properly scaled."],"metadata":{"id":"K81dQxMisdK5"}},{"cell_type":"code","source":["# Check the mean and standard deviation of the original and scaled data\n","original_mean = np.mean(X_train, axis=0)\n","original_std = np.std(X_train, axis=0)\n","\n","scaled_mean = np.mean(scaled_X_train, axis=0)\n","scaled_std = np.std(scaled_X_train, axis=0)\n","\n","print(\"\\nOriginal means:\\n\", original_mean)\n","print(\"Original standard deviations:\\n\", original_std)\n","\n","print(\"\\nScaled means (should be close to 0):\", scaled_mean)\n","print(\"Scaled standard deviations (should be close to 1):\", scaled_std)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"thyIypVqKlrO","executionInfo":{"status":"ok","timestamp":1721598285518,"user_tz":240,"elapsed":197,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"5db1e860-f271-483c-8896-215204bf6493"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Original means:\n"," customer_total_orders                            1.028266\n","customer_average_order_value                   199.302810\n","customer_tenure_days                             2.240099\n","customer_recency_days                          236.570468\n","customer_unique_products                         1.057782\n","customer_category_diversity                      1.021374\n","customer_average_product_price                 125.120254\n","customer_total_units                             1.192905\n","customer_average_delivery_time                  12.116452\n","customer_average_shipping_cost                  20.222332\n","customer_on_time_delivery_rate                   0.918476\n","customer_average_installments                    2.911403\n","customer_total_payment_transactions              1.301906\n","customer_preferred_payment_type_boleto           0.198364\n","customer_preferred_payment_type_credit_card      0.753785\n","customer_preferred_payment_type_debit_card       0.015231\n","customer_preferred_payment_type_voucher          0.032620\n","dtype: float64\n","Original standard deviations:\n"," customer_total_orders                            0.188878\n","customer_average_order_value                   555.208554\n","customer_tenure_days                            22.888368\n","customer_recency_days                          152.578035\n","customer_unique_products                         0.282074\n","customer_category_diversity                      0.154393\n","customer_average_product_price                 187.455972\n","customer_total_units                             0.715294\n","customer_average_delivery_time                   9.639040\n","customer_average_shipping_cost                  15.842573\n","customer_on_time_delivery_rate                   0.272431\n","customer_average_installments                    2.693412\n","customer_total_payment_transactions              3.484657\n","customer_preferred_payment_type_boleto           0.398768\n","customer_preferred_payment_type_credit_card      0.430805\n","customer_preferred_payment_type_debit_card       0.122472\n","customer_preferred_payment_type_voucher          0.177640\n","dtype: float64\n","\n","Scaled means (should be close to 0): [ 9.64499590e-17  6.72906691e-18  2.07946859e-17  6.20569504e-17\n"," -5.63092057e-18 -4.39258534e-18  5.79447428e-18 -6.38560412e-17\n"," -8.17184427e-17  1.48413309e-16 -1.38576722e-16 -1.05982804e-16\n","  1.69161265e-17  2.42994083e-18  5.15895130e-17  4.97203277e-17\n"," -1.44628209e-17]\n","Scaled standard deviations (should be close to 1): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"]}]},{"cell_type":"markdown","source":["We can conclude that, having observed the same scores for both scaled and unscaled data, the original features are on the same scale.\n","\n","## Alternative Models <a id=\"alternative_models\"></a>\n","\n","### Random Forest, Gradient Boosting, Lasso <a id=\"random_gradient_lasso\"></a>\n","\n","We will now evaluate the following models, selected for their diversity:\n","- Linear Regression (already evaluated) and Lasso are both linear models but with different regularizations.\n","- Random Forest Regressor and Gradient Boosting Regressor are both tree-based, but Random Forest is bagging while Gradient Boosting is boosting.\n"],"metadata":{"id":"3abDWm4hQD4U"}},{"cell_type":"code","source":["# Define the model pipeline using a parameter for switching models\n","pipeline = Pipeline([\n","    ('regressor', RandomForestRegressor(random_state=42))  # Placeholder, will be replaced in param_grid\n","])\n","\n","# Define a parameter grid with parameters for each of the models\n","param_grid = [\n","    {\n","        'regressor': [RandomForestRegressor(random_state=42)],\n","        'regressor__n_estimators': [100, 200],\n","        'regressor__max_depth': [10, 20, None]\n","    },\n","    {\n","        'regressor': [GradientBoostingRegressor(random_state=42)],\n","        'regressor__n_estimators': [100, 200],\n","        'regressor__max_depth': [3, 5, 7]\n","    },\n","    {\n","        'regressor': [Lasso(random_state=42)],\n","        'regressor__alpha': [0.1, 1, 10]\n","    }\n","]\n","\n","# Initialize GridSearchCV\n","grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=2)\n","\n","# Fit the model\n","grid_search.fit(X_train, y_train)\n","\n","# Print best hyperparameters\n","print(f'Best Hyperparameters: {grid_search.best_params_}')\n","\n","# Evaluate the best model\n","best_model = grid_search.best_estimator_\n","y_pred_best = best_model.predict(X_test)\n","\n","mse_best = mean_squared_error(y_test, y_pred_best)\n","r2_best = r2_score(y_test, y_pred_best)\n","\n","print('Best Model:')\n","print(f' Mean Squared Error: {mse_best}')\n","print(f' R^2 Score: {r2_best}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fplyLoc2YbTs","executionInfo":{"status":"ok","timestamp":1721599513641,"user_tz":240,"elapsed":1112178,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"d6156e9d-a995-4563-ddf3-5513e8a487f9"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Best Hyperparameters: {'regressor': Lasso(alpha=0.1, random_state=42), 'regressor__alpha': 0.1}\n","Best Model:\n"," Mean Squared Error: 2352.6644926868353\n"," R^2 Score: 0.9821256159179069\n"]}]},{"cell_type":"code","source":["# Gathering the results\n","results = grid_search.cv_results_\n","\n","# Looping through the results and printing them\n","for mean_score, params in zip(results['mean_test_score'], results['params']):\n","    print(f\"Score: {mean_score:.4f}, Parameters: {params}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XeRR5BvgYby_","executionInfo":{"status":"ok","timestamp":1721599513857,"user_tz":240,"elapsed":30,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"897eb45a-b021-4d94-a210-cc1fd8dc732b"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Score: 0.8975, Parameters: {'regressor': RandomForestRegressor(random_state=42), 'regressor__max_depth': 10, 'regressor__n_estimators': 100}\n","Score: 0.8971, Parameters: {'regressor': RandomForestRegressor(random_state=42), 'regressor__max_depth': 10, 'regressor__n_estimators': 200}\n","Score: 0.9003, Parameters: {'regressor': RandomForestRegressor(random_state=42), 'regressor__max_depth': 20, 'regressor__n_estimators': 100}\n","Score: 0.8984, Parameters: {'regressor': RandomForestRegressor(random_state=42), 'regressor__max_depth': 20, 'regressor__n_estimators': 200}\n","Score: 0.8999, Parameters: {'regressor': RandomForestRegressor(random_state=42), 'regressor__max_depth': None, 'regressor__n_estimators': 100}\n","Score: 0.9001, Parameters: {'regressor': RandomForestRegressor(random_state=42), 'regressor__max_depth': None, 'regressor__n_estimators': 200}\n","Score: 0.8798, Parameters: {'regressor': GradientBoostingRegressor(random_state=42), 'regressor__max_depth': 3, 'regressor__n_estimators': 100}\n","Score: 0.8802, Parameters: {'regressor': GradientBoostingRegressor(random_state=42), 'regressor__max_depth': 3, 'regressor__n_estimators': 200}\n","Score: 0.8841, Parameters: {'regressor': GradientBoostingRegressor(random_state=42), 'regressor__max_depth': 5, 'regressor__n_estimators': 100}\n","Score: 0.8842, Parameters: {'regressor': GradientBoostingRegressor(random_state=42), 'regressor__max_depth': 5, 'regressor__n_estimators': 200}\n","Score: 0.8942, Parameters: {'regressor': GradientBoostingRegressor(random_state=42), 'regressor__max_depth': 7, 'regressor__n_estimators': 100}\n","Score: 0.8942, Parameters: {'regressor': GradientBoostingRegressor(random_state=42), 'regressor__max_depth': 7, 'regressor__n_estimators': 200}\n","Score: 0.9750, Parameters: {'regressor': Lasso(alpha=0.1, random_state=42), 'regressor__alpha': 0.1}\n","Score: 0.9748, Parameters: {'regressor': Lasso(alpha=0.1, random_state=42), 'regressor__alpha': 1}\n","Score: 0.9709, Parameters: {'regressor': Lasso(alpha=0.1, random_state=42), 'regressor__alpha': 10}\n"]}]},{"cell_type":"markdown","source":["The Mean Test Score, the average score (e.g., accuracy, mean squared error) across all cross-validation folds for each parameter combination, resulted in the following best alternative models:\n","\n","- **Random Forest Regressor**: Score ≈ 0.9003\n","- **Gradient Boosting Regressor**: Score ≈ 0.8942\n","- **Lasso**: Score ≈ 0.9750\n","\n","We will use these, along with Linear Regression, for the ensemble and stacked models.\n","\n","## Ensemble Model <a id=\"ensemble\"></a>\n","\n","Ensemble models leverage the strength of multiple models to improve overall performance, mitigate individual model weaknesses, and enhance robustness and generalizability.\n","\n","\n"],"metadata":{"id":"2MLhtvJ34_1V"}},{"cell_type":"code","source":["# Define the ensemble model\n","ensemble_model = VotingRegressor(estimators=[\n","    ('lr', LinearRegression()),\n","    ('rf', RandomForestRegressor(max_depth=20, n_estimators=100, random_state=42)),\n","    ('gb', GradientBoostingRegressor(max_depth=7, n_estimators=100, random_state=42)),\n","    ('lasso', Lasso(alpha=0.1, random_state=42))\n","])\n","\n","# Train the ensemble model\n","ensemble_model.fit(X_train, y_train)\n","\n","# Evaluate ensemble model performance\n","ensemble_pred = ensemble_model.predict(X_test)\n","mse_ensemble = mean_squared_error(y_test, ensemble_pred)\n","r2_ensemble = r2_score(y_test, ensemble_pred)\n","print(f'Ensemble MSE: {mse_ensemble}, Ensemble R²: {r2_ensemble}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SuYELv5rYcld","executionInfo":{"status":"ok","timestamp":1721599601374,"user_tz":240,"elapsed":87529,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"37cc16ad-ae93-4d4f-e804-4eed35509007"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Ensemble MSE: 5810.8868198822665, Ensemble R²: 0.955851748857939\n"]}]},{"cell_type":"markdown","source":["## Stacked Model <a id=\"stacked\"></a>\n","\n","In a stacked model, we combine base models using another model (meta-model) to learn how to best combine the predictions."],"metadata":{"id":"dYRNG6gw8BES"}},{"cell_type":"code","source":["# Define base models\n","base_models = [\n","    ('lr', LinearRegression()),\n","    ('rf', RandomForestRegressor(max_depth=20, n_estimators=100, random_state=42)),\n","    ('gb', GradientBoostingRegressor(max_depth=7, n_estimators=100, random_state=42)),\n","    ('lasso', Lasso(alpha=0.1, random_state=42))\n","]\n","\n","# Define the meta-model (stacker), using a robust model like linear regression for simplicity\n","stacked_model = StackingRegressor(\n","    estimators=base_models,\n","    final_estimator=LinearRegression()\n",")\n","\n","# Train the stacked model\n","stacked_model.fit(X_train, y_train)\n","\n","# Evaluate stacked model performance\n","stacked_pred = stacked_model.predict(X_test)\n","mse_stacked = mean_squared_error(y_test, stacked_pred)\n","r2_stacked = r2_score(y_test, stacked_pred)\n","print(f'Stacked MSE: {mse_stacked}, Stacked R²: {r2_stacked}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0wVtdUuiYdRi","executionInfo":{"status":"ok","timestamp":1721600024803,"user_tz":240,"elapsed":423458,"user":{"displayName":"Michelle Doll","userId":"15715146737309096012"}},"outputId":"17f2a8ed-3ccc-43b5-800a-664ee08a1dd5"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Stacked MSE: 2403.328311494225, Stacked R²: 0.9817406972186007\n"]}]},{"cell_type":"markdown","source":["## Compare and Select the Best Model <a id=\"compare_select\"></a>\n","\n","Here are the best final scores of all of the models we evaluated:\n","\n","- **Linear Regression**: MSE = 2352.7929843740335,\n","R² = 0.9821246397014615\n","- **Alternative Models: Lasso**: MSE = 2352.6644926868353,\n"," R² = 0.9821256159179069\n","- **Ensemble**: MSE = 5810.8868198822665, R² = 0.955851748857939\n","- **Stacked**: MSE = 2403.328311494225, R² = 0.9817406972186007\n","\n","The Lasso model gave the best performance indicating that it is the best choice for overall predictive accuracy.\n","\n","Linear regression also performed well but was slightly outperformed.\n"],"metadata":{"id":"Ey8QjU1hMCcB"}},{"cell_type":"markdown","source":["## Summary <a id=\"summary\"></a>\n","\n","In this capstone project, the goal was to improve targeted marketing strategies in a Brazilian e-commerce setting by accurately predicting Customer Lifetime Value (CLV) using a dataset of 100,000 anonymized orders from 2016 to 2018. The key metric for prediction was `customer_total_spend`.\n","\n","Here we used pre-processing, training, and modeling to make a decision on the model that would have the best predictive power.\n","\n","Initially, in the data wrangling portion of our capstone, we created some dervied features using the entire dataset. However, we know that data leakage occurs when information from outside the training set is used to create or transform features, leading to overly optimistic performance estimates during model evaluation. Therefore, to prevent data leakage, we first took the original dataset and divided it into training and test subsets. Features were then recreated on each subset, ensuring a realistic scenario for predicting new data.\n","\n","One-hot encoding then fit the encoder on the training subset and transformed both the training and test subsets. The encoded features were merged back into the original datasets after dropping the original categorical columns.\n","\n","The data (excluding non-relevant features) was split into training and test sets for modeling, and standardization was fit on the training set and transformed both the training and test sets to scale the data.\n","\n","The modeling began with linear regression, which served as the baseline model. Evaluation was done on both scaled and non-scaled data, revealing identical performance, confirming the original features were on a consistent scale.\n","\n","Alternative models were also explored, including Random Forest, Gradient Boosting, and Lasso, which were chosen for their diverse approaches: Linear Regression (already evaluated) and Lasso are both linear models but with different regularizations. Random Forest Regressor and Gradient Boosting Regressor are both tree-based, but Random Forest is bagging while Gradient Boosting is boosting.\n","\n","Ensemble modeling aggregated predictions through voting, while stacked modeling used a meta-model to combine base model predictions.\n","\n","Comparing all final models, the Lasso model emerged as the most accurate, with a Mean Squared Error (MSE) of 2352.66 and an \\(R²\\) of 0.9821, closely followed by Linear Regression.\n","\n","In conclusion, the Lasso model provided the best predictive performance, marking it as the optimal model for predicting Customer Lifetime Value in this e-commerce context."],"metadata":{"id":"n7Y75zLfNFmW"}},{"cell_type":"code","source":["\n"],"metadata":{"id":"WBHLJoq-TVG5"},"execution_count":null,"outputs":[]}]}